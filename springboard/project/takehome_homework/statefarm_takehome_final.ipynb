{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Data Scientist Opening Pre-employment Assessment\n",
    "\n",
    "Rezoan Ahmed Shuvro\n",
    "email: rezoanshuvro@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries and loading the .csv file in a dataframe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "dataset = pd.read_csv('exercise_01_train.csv')\n",
    "test =pd.read_csv('exercise_01_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.142889</td>\n",
       "      <td>-15.675620</td>\n",
       "      <td>3.583176</td>\n",
       "      <td>-22.397489</td>\n",
       "      <td>27.221894</td>\n",
       "      <td>-34.110924</td>\n",
       "      <td>-0.072829</td>\n",
       "      <td>-0.544444</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>-2.691778</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916575</td>\n",
       "      <td>5.240820</td>\n",
       "      <td>euorpe</td>\n",
       "      <td>2.431170</td>\n",
       "      <td>0.454074</td>\n",
       "      <td>-18.572032</td>\n",
       "      <td>-14.291524</td>\n",
       "      <td>0.178579</td>\n",
       "      <td>18.110170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-52.214630</td>\n",
       "      <td>5.847135</td>\n",
       "      <td>-10.902843</td>\n",
       "      <td>-14.132351</td>\n",
       "      <td>20.588574</td>\n",
       "      <td>36.107322</td>\n",
       "      <td>0.115023</td>\n",
       "      <td>0.276093</td>\n",
       "      <td>-0.699168</td>\n",
       "      <td>-0.972708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370941</td>\n",
       "      <td>-3.794542</td>\n",
       "      <td>asia</td>\n",
       "      <td>2.592326</td>\n",
       "      <td>31.921833</td>\n",
       "      <td>3.317139</td>\n",
       "      <td>10.037003</td>\n",
       "      <td>-1.930870</td>\n",
       "      <td>-3.486898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.718500</td>\n",
       "      <td>2.064334</td>\n",
       "      <td>12.394186</td>\n",
       "      <td>-18.667102</td>\n",
       "      <td>47.465504</td>\n",
       "      <td>-50.373658</td>\n",
       "      <td>0.253707</td>\n",
       "      <td>1.068968</td>\n",
       "      <td>2.939713</td>\n",
       "      <td>2.691218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.449817</td>\n",
       "      <td>12.470532</td>\n",
       "      <td>asia</td>\n",
       "      <td>7.143821</td>\n",
       "      <td>9.401490</td>\n",
       "      <td>-10.604968</td>\n",
       "      <td>7.643215</td>\n",
       "      <td>-0.842198</td>\n",
       "      <td>-79.358236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-28.003111</td>\n",
       "      <td>8.565128</td>\n",
       "      <td>-8.592092</td>\n",
       "      <td>5.918960</td>\n",
       "      <td>-3.224154</td>\n",
       "      <td>78.315783</td>\n",
       "      <td>-0.879845</td>\n",
       "      <td>1.176889</td>\n",
       "      <td>-2.414752</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.274733</td>\n",
       "      <td>3.484450</td>\n",
       "      <td>asia</td>\n",
       "      <td>-4.998195</td>\n",
       "      <td>-20.312810</td>\n",
       "      <td>14.818524</td>\n",
       "      <td>-9.180674</td>\n",
       "      <td>1.356972</td>\n",
       "      <td>14.475681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.703016</td>\n",
       "      <td>30.736353</td>\n",
       "      <td>-30.101857</td>\n",
       "      <td>-21.201140</td>\n",
       "      <td>-91.946233</td>\n",
       "      <td>-47.469246</td>\n",
       "      <td>-0.646831</td>\n",
       "      <td>-0.578398</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>-1.426112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.644261</td>\n",
       "      <td>4.082783</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-29.334324</td>\n",
       "      <td>1.734433</td>\n",
       "      <td>-12.262072</td>\n",
       "      <td>-0.043228</td>\n",
       "      <td>-19.003881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0         x1         x2         x3         x4         x5        x6  \\\n",
       "0  10.142889 -15.675620   3.583176 -22.397489  27.221894 -34.110924 -0.072829   \n",
       "1 -52.214630   5.847135 -10.902843 -14.132351  20.588574  36.107322  0.115023   \n",
       "2  67.718500   2.064334  12.394186 -18.667102  47.465504 -50.373658  0.253707   \n",
       "3 -28.003111   8.565128  -8.592092   5.918960  -3.224154  78.315783 -0.879845   \n",
       "4  80.703016  30.736353 -30.101857 -21.201140 -91.946233 -47.469246 -0.646831   \n",
       "\n",
       "         x7        x8        x9 ...       x91        x92     x93       x94  \\\n",
       "0 -0.544444  0.997601 -2.691778 ...  1.916575   5.240820  euorpe  2.431170   \n",
       "1  0.276093 -0.699168 -0.972708 ...  0.370941  -3.794542    asia  2.592326   \n",
       "2  1.068968  2.939713  2.691218 ...  1.449817  12.470532    asia  7.143821   \n",
       "3  1.176889 -2.414752  0.589646 ... -3.274733   3.484450    asia -4.998195   \n",
       "4 -0.578398  0.980849 -1.426112 ... -0.644261   4.082783    asia -0.012556   \n",
       "\n",
       "         x95        x96        x97       x98        x99  y  \n",
       "0   0.454074 -18.572032 -14.291524  0.178579  18.110170  0  \n",
       "1  31.921833   3.317139  10.037003 -1.930870  -3.486898  0  \n",
       "2   9.401490 -10.604968   7.643215 -0.842198 -79.358236  0  \n",
       "3 -20.312810  14.818524  -9.180674  1.356972  14.475681  0  \n",
       "4 -29.334324   1.734433 -12.262072 -0.043228 -19.003881  0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head of the data set\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>50.168318</td>\n",
       "      <td>-4.272643</td>\n",
       "      <td>2.409248</td>\n",
       "      <td>-11.697615</td>\n",
       "      <td>39.234827</td>\n",
       "      <td>31.353302</td>\n",
       "      <td>1.416008</td>\n",
       "      <td>1.825775</td>\n",
       "      <td>2.027886</td>\n",
       "      <td>-3.753114</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.411384</td>\n",
       "      <td>-17.587621</td>\n",
       "      <td>america</td>\n",
       "      <td>6.278226</td>\n",
       "      <td>-18.743967</td>\n",
       "      <td>-8.067506</td>\n",
       "      <td>5.258203</td>\n",
       "      <td>-2.623772</td>\n",
       "      <td>-15.550075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>-8.653274</td>\n",
       "      <td>10.572796</td>\n",
       "      <td>1.377445</td>\n",
       "      <td>-21.472814</td>\n",
       "      <td>-42.686853</td>\n",
       "      <td>28.893360</td>\n",
       "      <td>3.379456</td>\n",
       "      <td>-1.241659</td>\n",
       "      <td>-0.040278</td>\n",
       "      <td>0.612898</td>\n",
       "      <td>...</td>\n",
       "      <td>7.622624</td>\n",
       "      <td>-6.473851</td>\n",
       "      <td>asia</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>-6.506186</td>\n",
       "      <td>12.434701</td>\n",
       "      <td>-6.001283</td>\n",
       "      <td>-5.340633</td>\n",
       "      <td>18.276723</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x0         x1        x2         x3         x4         x5  \\\n",
       "39998  50.168318  -4.272643  2.409248 -11.697615  39.234827  31.353302   \n",
       "39999  -8.653274  10.572796  1.377445 -21.472814 -42.686853  28.893360   \n",
       "\n",
       "             x6        x7        x8        x9 ...       x91        x92  \\\n",
       "39998  1.416008  1.825775  2.027886 -3.753114 ... -1.411384 -17.587621   \n",
       "39999  3.379456 -1.241659 -0.040278  0.612898 ...  7.622624  -6.473851   \n",
       "\n",
       "           x93       x94        x95        x96       x97       x98        x99  \\\n",
       "39998  america  6.278226 -18.743967  -8.067506  5.258203 -2.623772 -15.550075   \n",
       "39999     asia  0.055730  -6.506186  12.434701 -6.001283 -5.340633  18.276723   \n",
       "\n",
       "       y  \n",
       "39998  0  \n",
       "39999  1  \n",
       "\n",
       "[2 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tail of the dataset to check the size is okay\n",
    "dataset.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9',\n",
       "       ...\n",
       "       'x91', 'x92', 'x93', 'x94', 'x95', 'x96', 'x97', 'x98', 'x99', 'y'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns of the dataset\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.798825\n",
       "1    0.201175\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output variable. Output is imbalanced. 80% samples represent 0 class and 20% samples represent 1 class\n",
    "dataset.y.value_counts('normalize=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Columns: 101 entries, x0 to y\n",
      "dtypes: float64(94), int64(1), object(6)\n",
      "memory usage: 30.8+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39986.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39990.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39998.000000</td>\n",
       "      <td>39994.000000</td>\n",
       "      <td>39989.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39985.000000</td>\n",
       "      <td>39993.000000</td>\n",
       "      <td>39995.000000</td>\n",
       "      <td>39987.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.259955</td>\n",
       "      <td>-3.249786</td>\n",
       "      <td>1.030666</td>\n",
       "      <td>-0.747566</td>\n",
       "      <td>0.283820</td>\n",
       "      <td>-1.773510</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>-0.651093</td>\n",
       "      <td>-0.014688</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.274809</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>-0.007572</td>\n",
       "      <td>-0.629241</td>\n",
       "      <td>-1.986671</td>\n",
       "      <td>0.036482</td>\n",
       "      <td>1.486887</td>\n",
       "      <td>0.201175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.374182</td>\n",
       "      <td>15.171131</td>\n",
       "      <td>24.732185</td>\n",
       "      <td>15.225730</td>\n",
       "      <td>42.240018</td>\n",
       "      <td>42.124100</td>\n",
       "      <td>1.065955</td>\n",
       "      <td>3.382644</td>\n",
       "      <td>2.947472</td>\n",
       "      <td>1.906496</td>\n",
       "      <td>...</td>\n",
       "      <td>154.038206</td>\n",
       "      <td>3.311041</td>\n",
       "      <td>8.763944</td>\n",
       "      <td>4.979690</td>\n",
       "      <td>19.238210</td>\n",
       "      <td>16.915222</td>\n",
       "      <td>14.375663</td>\n",
       "      <td>5.633052</td>\n",
       "      <td>36.926796</td>\n",
       "      <td>0.400884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-140.780478</td>\n",
       "      <td>-64.493908</td>\n",
       "      <td>-105.388182</td>\n",
       "      <td>-63.804916</td>\n",
       "      <td>-158.195975</td>\n",
       "      <td>-169.237259</td>\n",
       "      <td>-4.133490</td>\n",
       "      <td>-12.966970</td>\n",
       "      <td>-12.037625</td>\n",
       "      <td>-7.446200</td>\n",
       "      <td>...</td>\n",
       "      <td>-674.004008</td>\n",
       "      <td>-12.807938</td>\n",
       "      <td>-38.121111</td>\n",
       "      <td>-21.578977</td>\n",
       "      <td>-87.669573</td>\n",
       "      <td>-77.010252</td>\n",
       "      <td>-57.709983</td>\n",
       "      <td>-23.588876</td>\n",
       "      <td>-154.559512</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-17.800204</td>\n",
       "      <td>-13.458580</td>\n",
       "      <td>-15.565461</td>\n",
       "      <td>-11.078276</td>\n",
       "      <td>-28.246509</td>\n",
       "      <td>-30.391354</td>\n",
       "      <td>-0.723098</td>\n",
       "      <td>-2.299081</td>\n",
       "      <td>-2.628856</td>\n",
       "      <td>-1.299759</td>\n",
       "      <td>...</td>\n",
       "      <td>-116.645845</td>\n",
       "      <td>-2.218739</td>\n",
       "      <td>-5.925508</td>\n",
       "      <td>-3.435180</td>\n",
       "      <td>-12.895717</td>\n",
       "      <td>-11.948902</td>\n",
       "      <td>-11.686033</td>\n",
       "      <td>-3.770599</td>\n",
       "      <td>-23.559519</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.354662</td>\n",
       "      <td>-3.386601</td>\n",
       "      <td>1.132995</td>\n",
       "      <td>-0.714888</td>\n",
       "      <td>0.292788</td>\n",
       "      <td>-1.753365</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>-0.003556</td>\n",
       "      <td>-0.659223</td>\n",
       "      <td>-0.028170</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.471306</td>\n",
       "      <td>-0.006726</td>\n",
       "      <td>0.009306</td>\n",
       "      <td>-0.037111</td>\n",
       "      <td>0.124945</td>\n",
       "      <td>-0.481374</td>\n",
       "      <td>-2.026059</td>\n",
       "      <td>0.041838</td>\n",
       "      <td>1.465346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.829780</td>\n",
       "      <td>6.881661</td>\n",
       "      <td>17.677615</td>\n",
       "      <td>9.552404</td>\n",
       "      <td>28.719663</td>\n",
       "      <td>26.844781</td>\n",
       "      <td>0.715844</td>\n",
       "      <td>2.259972</td>\n",
       "      <td>1.322101</td>\n",
       "      <td>1.263469</td>\n",
       "      <td>...</td>\n",
       "      <td>90.101751</td>\n",
       "      <td>2.238996</td>\n",
       "      <td>5.909011</td>\n",
       "      <td>3.299108</td>\n",
       "      <td>12.988509</td>\n",
       "      <td>10.793171</td>\n",
       "      <td>7.611660</td>\n",
       "      <td>3.840100</td>\n",
       "      <td>26.548474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>177.399176</td>\n",
       "      <td>62.906822</td>\n",
       "      <td>99.394915</td>\n",
       "      <td>59.338352</td>\n",
       "      <td>179.342581</td>\n",
       "      <td>170.894497</td>\n",
       "      <td>5.311653</td>\n",
       "      <td>16.619445</td>\n",
       "      <td>14.994937</td>\n",
       "      <td>7.300186</td>\n",
       "      <td>...</td>\n",
       "      <td>603.911528</td>\n",
       "      <td>14.982369</td>\n",
       "      <td>35.785334</td>\n",
       "      <td>20.983463</td>\n",
       "      <td>78.785164</td>\n",
       "      <td>70.182932</td>\n",
       "      <td>60.481075</td>\n",
       "      <td>22.759016</td>\n",
       "      <td>143.126382</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x0            x1            x2            x3            x4  \\\n",
       "count  39986.000000  39990.000000  39994.000000  39990.000000  39994.000000   \n",
       "mean       8.259955     -3.249786      1.030666     -0.747566      0.283820   \n",
       "std       38.374182     15.171131     24.732185     15.225730     42.240018   \n",
       "min     -140.780478    -64.493908   -105.388182    -63.804916   -158.195975   \n",
       "25%      -17.800204    -13.458580    -15.565461    -11.078276    -28.246509   \n",
       "50%        8.354662     -3.386601      1.132995     -0.714888      0.292788   \n",
       "75%       33.829780      6.881661     17.677615      9.552404     28.719663   \n",
       "max      177.399176     62.906822     99.394915     59.338352    179.342581   \n",
       "\n",
       "                 x5            x6            x7            x8            x9  \\\n",
       "count  39990.000000  39993.000000  39987.000000  39994.000000  39993.000000   \n",
       "mean      -1.773510     -0.000232     -0.016107     -0.651093     -0.014688   \n",
       "std       42.124100      1.065955      3.382644      2.947472      1.906496   \n",
       "min     -169.237259     -4.133490    -12.966970    -12.037625     -7.446200   \n",
       "25%      -30.391354     -0.723098     -2.299081     -2.628856     -1.299759   \n",
       "50%       -1.753365      0.001105     -0.003556     -0.659223     -0.028170   \n",
       "75%       26.844781      0.715844      2.259972      1.322101      1.263469   \n",
       "max      170.894497      5.311653     16.619445     14.994937      7.300186   \n",
       "\n",
       "           ...                x90           x91           x92           x94  \\\n",
       "count      ...       39993.000000  39998.000000  39994.000000  39989.000000   \n",
       "mean       ...         -14.274809      0.011390      0.003948     -0.050510   \n",
       "std        ...         154.038206      3.311041      8.763944      4.979690   \n",
       "min        ...        -674.004008    -12.807938    -38.121111    -21.578977   \n",
       "25%        ...        -116.645845     -2.218739     -5.925508     -3.435180   \n",
       "50%        ...         -11.471306     -0.006726      0.009306     -0.037111   \n",
       "75%        ...          90.101751      2.238996      5.909011      3.299108   \n",
       "max        ...         603.911528     14.982369     35.785334     20.983463   \n",
       "\n",
       "                x95           x96           x97           x98           x99  \\\n",
       "count  39993.000000  39985.000000  39993.000000  39995.000000  39987.000000   \n",
       "mean      -0.007572     -0.629241     -1.986671      0.036482      1.486887   \n",
       "std       19.238210     16.915222     14.375663      5.633052     36.926796   \n",
       "min      -87.669573    -77.010252    -57.709983    -23.588876   -154.559512   \n",
       "25%      -12.895717    -11.948902    -11.686033     -3.770599    -23.559519   \n",
       "50%        0.124945     -0.481374     -2.026059      0.041838      1.465346   \n",
       "75%       12.988509     10.793171      7.611660      3.840100     26.548474   \n",
       "max       78.785164     70.182932     60.481075     22.759016    143.126382   \n",
       "\n",
       "                  y  \n",
       "count  40000.000000  \n",
       "mean       0.201175  \n",
       "std        0.400884  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 95 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First checking the classification using Logistic regression without any tuning, removing all the missing values and without any preprocessing. This will be my base accuracy which i will improve subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      6392\n",
      "           1       0.80      0.61      0.69      1608\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      8000\n",
      "   macro avg       0.85      0.79      0.81      8000\n",
      "weighted avg       0.89      0.89      0.88      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset[dataset==np.inf]=np.nan\n",
    "dataset.fillna(dataset.mean(),inplace=True)\n",
    "X =dataset.drop(['x34','x35','x68','x93','x41','x45','y'],axis=1)\n",
    "y = dataset['y']\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "#training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# prediction using model\n",
    "from sklearn.metrics import accuracy_score\n",
    "prediction = logmodel.predict(X_test)\n",
    "#evaluation(Accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(prediction,y_test))\n",
    "#evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalance in class, accuracy is high but the recall and f1-score is low. For this dataset, our metric should be recall/f1-score than accuracy. Base f1-score is 0.69\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.01456013e-03,  4.30316858e-01,  1.35901214e-01,\n",
       "        -2.66219022e-01,  6.02327509e-02, -7.13458380e-02,\n",
       "        -5.40569174e-03, -1.48344131e-02, -4.34590294e-01,\n",
       "         2.57120714e-02, -5.79191196e-01, -3.20165691e-05,\n",
       "        -7.36871570e-01,  7.68410647e-03, -3.96408828e-02,\n",
       "         1.50687548e-02,  8.68193588e-03, -1.41536502e-02,\n",
       "        -1.21439551e-01,  2.28686513e-01, -2.96120871e-01,\n",
       "        -4.17653433e-01, -3.89955224e-02,  5.14367059e-04,\n",
       "        -1.59067113e-02,  4.05608614e-02, -8.47889280e-03,\n",
       "         2.81940112e-01,  2.78561074e-03, -2.36600900e-02,\n",
       "        -1.95050545e-02, -2.27600029e-02,  2.54537193e-04,\n",
       "         1.24398258e-01,  1.74449092e-02, -8.21442885e-01,\n",
       "         4.97526654e-02,  1.45646394e-02,  3.90238587e-01,\n",
       "         6.71444858e-01, -3.43083103e-01,  1.68317792e+00,\n",
       "         2.81398018e-03, -1.48306085e-01, -1.01843447e-02,\n",
       "         1.62253412e-01, -8.06722032e-01,  4.58456552e-01,\n",
       "         3.30287686e-02, -3.10942916e-01,  9.28351822e-03,\n",
       "         1.88223541e-02, -4.92159916e-01,  4.67320556e-01,\n",
       "         9.30521336e-01, -2.28842728e-02,  1.80154125e-02,\n",
       "         4.36831542e-02,  1.57726060e-02,  5.46084851e-01,\n",
       "        -2.28593550e-02, -5.01349639e-03, -2.80078897e-02,\n",
       "         6.03259432e-03,  3.57157334e-01,  1.36742402e-01,\n",
       "        -8.90224042e-01, -4.44519141e-01,  2.85582446e-01,\n",
       "         1.43894164e-01, -9.85253119e-01, -2.05251833e-02,\n",
       "         4.54376056e-01,  6.09268981e-02,  1.41548876e-01,\n",
       "         2.72908378e-01, -3.05382722e-02, -1.61138827e-02,\n",
       "         7.48390964e-01, -3.05272082e-02,  7.04408868e-01,\n",
       "        -3.24720321e-02, -1.19896539e-02,  1.48520431e-04,\n",
       "         2.02903254e-02, -1.21290854e+00, -1.66534953e-02,\n",
       "        -8.03068553e-03,  5.61454172e-02, -3.60027670e-02,\n",
       "        -2.05783531e-01,  1.17991988e+00,  4.86006893e-02,\n",
       "         2.03233009e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beta co-efficients\n",
    "logmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.142889</td>\n",
       "      <td>-15.675620</td>\n",
       "      <td>3.583176</td>\n",
       "      <td>-22.397489</td>\n",
       "      <td>27.221894</td>\n",
       "      <td>-34.110924</td>\n",
       "      <td>-0.072829</td>\n",
       "      <td>-0.544444</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>-2.691778</td>\n",
       "      <td>...</td>\n",
       "      <td>1.916575</td>\n",
       "      <td>5.240820</td>\n",
       "      <td>euorpe</td>\n",
       "      <td>2.431170</td>\n",
       "      <td>0.454074</td>\n",
       "      <td>-18.572032</td>\n",
       "      <td>-14.291524</td>\n",
       "      <td>0.178579</td>\n",
       "      <td>18.110170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-52.214630</td>\n",
       "      <td>5.847135</td>\n",
       "      <td>-10.902843</td>\n",
       "      <td>-14.132351</td>\n",
       "      <td>20.588574</td>\n",
       "      <td>36.107322</td>\n",
       "      <td>0.115023</td>\n",
       "      <td>0.276093</td>\n",
       "      <td>-0.699168</td>\n",
       "      <td>-0.972708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370941</td>\n",
       "      <td>-3.794542</td>\n",
       "      <td>asia</td>\n",
       "      <td>2.592326</td>\n",
       "      <td>31.921833</td>\n",
       "      <td>3.317139</td>\n",
       "      <td>10.037003</td>\n",
       "      <td>-1.930870</td>\n",
       "      <td>-3.486898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.718500</td>\n",
       "      <td>2.064334</td>\n",
       "      <td>12.394186</td>\n",
       "      <td>-18.667102</td>\n",
       "      <td>47.465504</td>\n",
       "      <td>-50.373658</td>\n",
       "      <td>0.253707</td>\n",
       "      <td>1.068968</td>\n",
       "      <td>2.939713</td>\n",
       "      <td>2.691218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.449817</td>\n",
       "      <td>12.470532</td>\n",
       "      <td>asia</td>\n",
       "      <td>7.143821</td>\n",
       "      <td>9.401490</td>\n",
       "      <td>-10.604968</td>\n",
       "      <td>7.643215</td>\n",
       "      <td>-0.842198</td>\n",
       "      <td>-79.358236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-28.003111</td>\n",
       "      <td>8.565128</td>\n",
       "      <td>-8.592092</td>\n",
       "      <td>5.918960</td>\n",
       "      <td>-3.224154</td>\n",
       "      <td>78.315783</td>\n",
       "      <td>-0.879845</td>\n",
       "      <td>1.176889</td>\n",
       "      <td>-2.414752</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.274733</td>\n",
       "      <td>3.484450</td>\n",
       "      <td>asia</td>\n",
       "      <td>-4.998195</td>\n",
       "      <td>-20.312810</td>\n",
       "      <td>14.818524</td>\n",
       "      <td>-9.180674</td>\n",
       "      <td>1.356972</td>\n",
       "      <td>14.475681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.703016</td>\n",
       "      <td>30.736353</td>\n",
       "      <td>-30.101857</td>\n",
       "      <td>-21.201140</td>\n",
       "      <td>-91.946233</td>\n",
       "      <td>-47.469246</td>\n",
       "      <td>-0.646831</td>\n",
       "      <td>-0.578398</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>-1.426112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.644261</td>\n",
       "      <td>4.082783</td>\n",
       "      <td>asia</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>-29.334324</td>\n",
       "      <td>1.734433</td>\n",
       "      <td>-12.262072</td>\n",
       "      <td>-0.043228</td>\n",
       "      <td>-19.003881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0         x1         x2         x3         x4         x5        x6  \\\n",
       "0  10.142889 -15.675620   3.583176 -22.397489  27.221894 -34.110924 -0.072829   \n",
       "1 -52.214630   5.847135 -10.902843 -14.132351  20.588574  36.107322  0.115023   \n",
       "2  67.718500   2.064334  12.394186 -18.667102  47.465504 -50.373658  0.253707   \n",
       "3 -28.003111   8.565128  -8.592092   5.918960  -3.224154  78.315783 -0.879845   \n",
       "4  80.703016  30.736353 -30.101857 -21.201140 -91.946233 -47.469246 -0.646831   \n",
       "\n",
       "         x7        x8        x9 ...       x91        x92     x93       x94  \\\n",
       "0 -0.544444  0.997601 -2.691778 ...  1.916575   5.240820  euorpe  2.431170   \n",
       "1  0.276093 -0.699168 -0.972708 ...  0.370941  -3.794542    asia  2.592326   \n",
       "2  1.068968  2.939713  2.691218 ...  1.449817  12.470532    asia  7.143821   \n",
       "3  1.176889 -2.414752  0.589646 ... -3.274733   3.484450    asia -4.998195   \n",
       "4 -0.578398  0.980849 -1.426112 ... -0.644261   4.082783    asia -0.012556   \n",
       "\n",
       "         x95        x96        x97       x98        x99  y  \n",
       "0   0.454074 -18.572032 -14.291524  0.178579  18.110170  0  \n",
       "1  31.921833   3.317139  10.037003 -1.930870  -3.486898  0  \n",
       "2   9.401490 -10.604968   7.643215 -0.842198 -79.358236  0  \n",
       "3 -20.312810  14.818524  -9.180674  1.356972  14.475681  0  \n",
       "4 -29.334324   1.734433 -12.262072 -0.043228 -19.003881  0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in each column using the mean of than column\n",
    "dataset.fillna(dataset.mean(),inplace=True)\n",
    "test.fillna(test.mean(),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in the missing values in the categorical columns using the largest value\n",
    "dataset= dataset.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "test = test.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value in dataset: 0\n",
      "missing value in testset: 0\n"
     ]
    }
   ],
   "source": [
    "# check if there is any missing value lest after imputing the missing values\n",
    "print('missing value in dataset:',dataset.isnull().sum().sum())\n",
    "print('missing value in testset:',test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in the dataset but very few in each column. I will fill the missing values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0     float64\n",
       "x1     float64\n",
       "x2     float64\n",
       "x3     float64\n",
       "x4     float64\n",
       "x5     float64\n",
       "x6     float64\n",
       "x7     float64\n",
       "x8     float64\n",
       "x9     float64\n",
       "x10    float64\n",
       "x11    float64\n",
       "x12    float64\n",
       "x13    float64\n",
       "x14    float64\n",
       "x15    float64\n",
       "x16    float64\n",
       "x17    float64\n",
       "x18    float64\n",
       "x19    float64\n",
       "x20    float64\n",
       "x21    float64\n",
       "x22    float64\n",
       "x23    float64\n",
       "x24    float64\n",
       "x25    float64\n",
       "x26    float64\n",
       "x27    float64\n",
       "x28    float64\n",
       "x29    float64\n",
       "        ...   \n",
       "x71    float64\n",
       "x72    float64\n",
       "x73    float64\n",
       "x74    float64\n",
       "x75    float64\n",
       "x76    float64\n",
       "x77    float64\n",
       "x78    float64\n",
       "x79    float64\n",
       "x80    float64\n",
       "x81    float64\n",
       "x82    float64\n",
       "x83    float64\n",
       "x84    float64\n",
       "x85    float64\n",
       "x86    float64\n",
       "x87    float64\n",
       "x88    float64\n",
       "x89    float64\n",
       "x90    float64\n",
       "x91    float64\n",
       "x92    float64\n",
       "x93     object\n",
       "x94    float64\n",
       "x95    float64\n",
       "x96    float64\n",
       "x97    float64\n",
       "x98    float64\n",
       "x99    float64\n",
       "y        int64\n",
       "Length: 101, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the data types of the DataFrame\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x35        y\n",
       "fri        0    0.666667\n",
       "           1    0.333333\n",
       "friday     0    0.823194\n",
       "           1    0.176806\n",
       "monday     0    0.553571\n",
       "           1    0.446429\n",
       "thur       0    0.836991\n",
       "           1    0.163009\n",
       "thurday    0    0.828281\n",
       "           1    0.171719\n",
       "tuesday    0    0.652314\n",
       "           1    0.347686\n",
       "wed        0    0.792037\n",
       "           1    0.207963\n",
       "wednesday  0    0.744526\n",
       "           1    0.255474\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column x35 represents the days.\n",
    "dataset.groupby('x35').y.value_counts('normalize=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volkswagon    12579\n",
       "Toyota        10946\n",
       "bmw            7304\n",
       "Honda          5129\n",
       "tesla          2275\n",
       "chrystler      1223\n",
       "nissan          336\n",
       "ford            165\n",
       "mercades         32\n",
       "chevrolet        11\n",
       "Name: x34, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x34 represents various counts of car\n",
    "dataset.x34.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wed          14844\n",
       "thurday      13359\n",
       "wednesday     5891\n",
       "thur          4374\n",
       "tuesday        929\n",
       "friday         526\n",
       "monday          56\n",
       "fri             21\n",
       "Name: x35, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#date counts\n",
    "dataset.x35.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "July       11097\n",
       "Jun         9279\n",
       "Aug         8155\n",
       "May         4788\n",
       "sept.       3466\n",
       "Apr         1641\n",
       "Oct          926\n",
       "Mar          409\n",
       "Nov          156\n",
       "Feb           54\n",
       "Dev           18\n",
       "January       11\n",
       "Name: x68, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#month counts\n",
    "dataset.x68.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asia       35416\n",
       "america     3136\n",
       "euorpe      1448\n",
       "Name: x93, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continent counts\n",
    "dataset.x93.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variables using one-hot encoder\n",
    "\n",
    "dum_df = pd.get_dummies(data=dataset, columns=['x34','x35','x68','x93'], prefix=['x34_','x35_','x68_','x93_'])\n",
    "\n",
    "test_df = pd.get_dummies(data=test, columns=['x34','x35','x68','x93'], prefix=['x34_','x35_','x68_','x93_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x68__July</th>\n",
       "      <th>x68__Jun</th>\n",
       "      <th>x68__Mar</th>\n",
       "      <th>x68__May</th>\n",
       "      <th>x68__Nov</th>\n",
       "      <th>x68__Oct</th>\n",
       "      <th>x68__sept.</th>\n",
       "      <th>x93__america</th>\n",
       "      <th>x93__asia</th>\n",
       "      <th>x93__euorpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.142889</td>\n",
       "      <td>-15.675620</td>\n",
       "      <td>3.583176</td>\n",
       "      <td>-22.397489</td>\n",
       "      <td>27.221894</td>\n",
       "      <td>-34.110924</td>\n",
       "      <td>-0.072829</td>\n",
       "      <td>-0.544444</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>-2.691778</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-52.214630</td>\n",
       "      <td>5.847135</td>\n",
       "      <td>-10.902843</td>\n",
       "      <td>-14.132351</td>\n",
       "      <td>20.588574</td>\n",
       "      <td>36.107322</td>\n",
       "      <td>0.115023</td>\n",
       "      <td>0.276093</td>\n",
       "      <td>-0.699168</td>\n",
       "      <td>-0.972708</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.718500</td>\n",
       "      <td>2.064334</td>\n",
       "      <td>12.394186</td>\n",
       "      <td>-18.667102</td>\n",
       "      <td>47.465504</td>\n",
       "      <td>-50.373658</td>\n",
       "      <td>0.253707</td>\n",
       "      <td>1.068968</td>\n",
       "      <td>2.939713</td>\n",
       "      <td>2.691218</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-28.003111</td>\n",
       "      <td>8.565128</td>\n",
       "      <td>-8.592092</td>\n",
       "      <td>5.918960</td>\n",
       "      <td>-3.224154</td>\n",
       "      <td>78.315783</td>\n",
       "      <td>-0.879845</td>\n",
       "      <td>1.176889</td>\n",
       "      <td>-2.414752</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.703016</td>\n",
       "      <td>30.736353</td>\n",
       "      <td>-30.101857</td>\n",
       "      <td>-21.201140</td>\n",
       "      <td>-91.946233</td>\n",
       "      <td>-47.469246</td>\n",
       "      <td>-0.646831</td>\n",
       "      <td>-0.578398</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>-1.426112</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x0         x1         x2         x3         x4         x5        x6  \\\n",
       "0  10.142889 -15.675620   3.583176 -22.397489  27.221894 -34.110924 -0.072829   \n",
       "1 -52.214630   5.847135 -10.902843 -14.132351  20.588574  36.107322  0.115023   \n",
       "2  67.718500   2.064334  12.394186 -18.667102  47.465504 -50.373658  0.253707   \n",
       "3 -28.003111   8.565128  -8.592092   5.918960  -3.224154  78.315783 -0.879845   \n",
       "4  80.703016  30.736353 -30.101857 -21.201140 -91.946233 -47.469246 -0.646831   \n",
       "\n",
       "         x7        x8        x9     ...       x68__July  x68__Jun  x68__Mar  \\\n",
       "0 -0.544444  0.997601 -2.691778     ...               0         1         0   \n",
       "1  0.276093 -0.699168 -0.972708     ...               1         0         0   \n",
       "2  1.068968  2.939713  2.691218     ...               1         0         0   \n",
       "3  1.176889 -2.414752  0.589646     ...               0         0         0   \n",
       "4 -0.578398  0.980849 -1.426112     ...               1         0         0   \n",
       "\n",
       "   x68__May  x68__Nov  x68__Oct  x68__sept.  x93__america  x93__asia  \\\n",
       "0         0         0         0           0             0          0   \n",
       "1         0         0         0           0             0          1   \n",
       "2         0         0         0           0             0          1   \n",
       "3         1         0         0           0             0          1   \n",
       "4         0         0         0           0             0          1   \n",
       "\n",
       "   x93__euorpe  \n",
       "0            1  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -1073.61\n",
       "1       1775.77\n",
       "2        697.23\n",
       "3       -134.48\n",
       "4       1195.16\n",
       "5      -1424.68\n",
       "6       -617.48\n",
       "7       1022.59\n",
       "8       1041.35\n",
       "9        272.05\n",
       "10       977.73\n",
       "11     -2040.44\n",
       "12      1288.77\n",
       "13     -1496.64\n",
       "14      -477.55\n",
       "15       472.64\n",
       "16       399.79\n",
       "17      -301.75\n",
       "18       663.79\n",
       "19      -270.93\n",
       "20      1890.87\n",
       "21      -269.77\n",
       "22       783.73\n",
       "23       639.48\n",
       "24      -529.72\n",
       "25      -191.83\n",
       "26     -1362.41\n",
       "27      1663.55\n",
       "28      -543.37\n",
       "29       694.77\n",
       "         ...   \n",
       "9970    -452.28\n",
       "9971    2106.34\n",
       "9972     410.84\n",
       "9973   -1116.36\n",
       "9974     622.79\n",
       "9975    -513.71\n",
       "9976    -551.90\n",
       "9977   -1431.80\n",
       "9978   -1000.70\n",
       "9979    -704.58\n",
       "9980     287.84\n",
       "9981     575.40\n",
       "9982    -602.24\n",
       "9983      43.54\n",
       "9984    -134.64\n",
       "9985    -286.30\n",
       "9986   -1813.54\n",
       "9987    -462.39\n",
       "9988     424.59\n",
       "9989   -1058.33\n",
       "9990     694.72\n",
       "9991    -465.20\n",
       "9992    1145.35\n",
       "9993     -94.50\n",
       "9994     317.62\n",
       "9995    -197.68\n",
       "9996    2084.39\n",
       "9997    -128.47\n",
       "9998     293.73\n",
       "9999    1202.18\n",
       "Name: x41, Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert currency to float (column x41))\n",
    "\n",
    "dum_df['x41'] = dum_df['x41'].str.replace(',', '')\n",
    "dum_df['x41'] = dum_df['x41'].str.replace('$', '')\n",
    "dum_df.x41.astype(float)\n",
    "\n",
    "test_df['x41'] = test_df['x41'].str.replace(',', '')\n",
    "test_df['x41'] = test_df['x41'].str.replace('$', '')\n",
    "test_df.x41.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -1073.61\n",
       "1       1775.77\n",
       "2        697.23\n",
       "3       -134.48\n",
       "4       1195.16\n",
       "5      -1424.68\n",
       "6       -617.48\n",
       "7       1022.59\n",
       "8       1041.35\n",
       "9        272.05\n",
       "10       977.73\n",
       "11     -2040.44\n",
       "12      1288.77\n",
       "13     -1496.64\n",
       "14      -477.55\n",
       "15       472.64\n",
       "16       399.79\n",
       "17      -301.75\n",
       "18       663.79\n",
       "19      -270.93\n",
       "20      1890.87\n",
       "21      -269.77\n",
       "22       783.73\n",
       "23       639.48\n",
       "24      -529.72\n",
       "25      -191.83\n",
       "26     -1362.41\n",
       "27      1663.55\n",
       "28      -543.37\n",
       "29       694.77\n",
       "         ...   \n",
       "9970    -452.28\n",
       "9971    2106.34\n",
       "9972     410.84\n",
       "9973   -1116.36\n",
       "9974     622.79\n",
       "9975    -513.71\n",
       "9976    -551.90\n",
       "9977   -1431.80\n",
       "9978   -1000.70\n",
       "9979    -704.58\n",
       "9980     287.84\n",
       "9981     575.40\n",
       "9982    -602.24\n",
       "9983      43.54\n",
       "9984    -134.64\n",
       "9985    -286.30\n",
       "9986   -1813.54\n",
       "9987    -462.39\n",
       "9988     424.59\n",
       "9989   -1058.33\n",
       "9990     694.72\n",
       "9991    -465.20\n",
       "9992    1145.35\n",
       "9993     -94.50\n",
       "9994     317.62\n",
       "9995    -197.68\n",
       "9996    2084.39\n",
       "9997    -128.47\n",
       "9998     293.73\n",
       "9999    1202.18\n",
       "Name: x41, Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert % to float (column x41))\n",
    "dum_df['x45'] = dum_df['x45'].str.replace(',', '')\n",
    "dum_df['x45'] = dum_df['x45'].str.replace('%', '')\n",
    "dum_df.x41.astype(float)\n",
    "\n",
    "test_df['x45'] = dum_df['x45'].str.replace(',', '')\n",
    "test_df['x45'] = dum_df['x45'].str.replace('%', '')\n",
    "test_df.x41.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.798825\n",
       "1    0.201175\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dum_df.y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, i will do upsampling to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling\n",
    "\n",
    "X =dum_df.drop(['y'],axis=1)\n",
    "y = dum_df['y']\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "zero = X[X.y==0]\n",
    "one = X[X.y==1]\n",
    "\n",
    "\n",
    "# upsample minority\n",
    "from sklearn.utils import resample\n",
    "one_upsampled = resample(one,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(zero), # match number in majority class\n",
    "                          random_state=101) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([zero, one_upsampled])\n",
    "\n",
    "\n",
    "\n",
    "X =upsampled.drop(['y'],axis=1)\n",
    "y = upsampled['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5\n",
       "0    0.5\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if balanced\n",
    "upsampled.y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 130 columns. I will do PCA to reduce the dimensionality of the dataset to 10 from 130. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "X =upsampled.drop(['y'],axis=1)\n",
    "y = upsampled['y']\n",
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_pca = sc.fit_transform(X)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X_pca)\n",
    "x_pca = pca.transform(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using reduced dimensionality and 20 features i have achieved 0.72 f1-score with logistic regression and 0.96 f1-score using random forest model. Since the f1-score decreased after using PCA, i decided against using PCA. I rollback to the original X,y i created after upsampling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now clean and ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, i will again implement the logistic regression model to check the f1-score. Recall that the base f1-score without any data cleaning was 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8308068459657701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      5119\n",
      "           1       0.83      0.83      0.83      5106\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     10225\n",
      "   macro avg       0.83      0.83      0.83     10225\n",
      "weighted avg       0.83      0.83      0.83     10225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "#training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# prediction using model\n",
    "from sklearn.metrics import accuracy_score\n",
    "prediction = logmodel.predict(X_test)\n",
    "#evaluation(Accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(prediction,y_test))\n",
    "#evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score now is 0.83, which is a significant improvement compared to 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now i will use the following two algorithms to classify with better accuracy and tune the hyperparameters to improve the accuracy\n",
    "\n",
    "1. Random forest (using sklearn library)\n",
    "2. Artificial Neural Netwrok (using Tensorflow with Keras wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'criterion': 'entropy', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Accuracy: 0.9908068459657702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      5119\n",
      "           1       0.99      0.99      0.99      5106\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     10225\n",
      "   macro avg       0.99      0.99      0.99     10225\n",
      "weighted avg       0.99      0.99      0.99     10225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with hyperparameter tuning parameters\n",
    "# importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#making the instance\n",
    "model=RandomForestClassifier()\n",
    "\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[50,150,300],\n",
    "          'min_samples_leaf':[5,10],\n",
    "          'min_samples_split':[5,10]}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model = GridSearchCV(model, param_grid=params)\n",
    "\n",
    "#Learning\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model.best_params_)\n",
    "\n",
    "#Prediction\n",
    "prediction=model.predict(X_test)\n",
    "\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\n",
    "\n",
    "#evaluation(Confusion Metrix)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using random forest with hyperprameter tuning the f1-score on the X_test is 0.99 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: x37                  Importance: 0.04\n",
      "Variable: x75                  Importance: 0.04\n",
      "Variable: x41                  Importance: 0.03\n",
      "Variable: x58                  Importance: 0.03\n",
      "Variable: x97                  Importance: 0.03\n",
      "Variable: x1                   Importance: 0.02\n",
      "Variable: x21                  Importance: 0.02\n",
      "Variable: x40                  Importance: 0.02\n",
      "Variable: x44                  Importance: 0.02\n",
      "Variable: x51                  Importance: 0.02\n",
      "Variable: x53                  Importance: 0.02\n",
      "Variable: x63                  Importance: 0.02\n",
      "Variable: x66                  Importance: 0.02\n",
      "Variable: x70                  Importance: 0.02\n",
      "Variable: x72                  Importance: 0.02\n",
      "Variable: x83                  Importance: 0.02\n",
      "Variable: x96                  Importance: 0.02\n",
      "Variable: x99                  Importance: 0.02\n",
      "Variable: x0                   Importance: 0.01\n",
      "Variable: x2                   Importance: 0.01\n",
      "Variable: x3                   Importance: 0.01\n",
      "Variable: x4                   Importance: 0.01\n",
      "Variable: x5                   Importance: 0.01\n",
      "Variable: x8                   Importance: 0.01\n",
      "Variable: x10                  Importance: 0.01\n",
      "Variable: x12                  Importance: 0.01\n",
      "Variable: x18                  Importance: 0.01\n",
      "Variable: x19                  Importance: 0.01\n",
      "Variable: x20                  Importance: 0.01\n",
      "Variable: x22                  Importance: 0.01\n",
      "Variable: x25                  Importance: 0.01\n",
      "Variable: x27                  Importance: 0.01\n",
      "Variable: x33                  Importance: 0.01\n",
      "Variable: x42                  Importance: 0.01\n",
      "Variable: x43                  Importance: 0.01\n",
      "Variable: x45                  Importance: 0.01\n",
      "Variable: x47                  Importance: 0.01\n",
      "Variable: x49                  Importance: 0.01\n",
      "Variable: x50                  Importance: 0.01\n",
      "Variable: x56                  Importance: 0.01\n",
      "Variable: x57                  Importance: 0.01\n",
      "Variable: x61                  Importance: 0.01\n",
      "Variable: x69                  Importance: 0.01\n",
      "Variable: x71                  Importance: 0.01\n",
      "Variable: x73                  Importance: 0.01\n",
      "Variable: x74                  Importance: 0.01\n",
      "Variable: x77                  Importance: 0.01\n",
      "Variable: x78                  Importance: 0.01\n",
      "Variable: x79                  Importance: 0.01\n",
      "Variable: x80                  Importance: 0.01\n",
      "Variable: x85                  Importance: 0.01\n",
      "Variable: x90                  Importance: 0.01\n",
      "Variable: x95                  Importance: 0.01\n",
      "Variable: x6                   Importance: 0.0\n",
      "Variable: x7                   Importance: 0.0\n",
      "Variable: x9                   Importance: 0.0\n",
      "Variable: x11                  Importance: 0.0\n",
      "Variable: x13                  Importance: 0.0\n",
      "Variable: x14                  Importance: 0.0\n",
      "Variable: x15                  Importance: 0.0\n",
      "Variable: x16                  Importance: 0.0\n",
      "Variable: x17                  Importance: 0.0\n",
      "Variable: x23                  Importance: 0.0\n",
      "Variable: x24                  Importance: 0.0\n",
      "Variable: x26                  Importance: 0.0\n",
      "Variable: x28                  Importance: 0.0\n",
      "Variable: x29                  Importance: 0.0\n",
      "Variable: x30                  Importance: 0.0\n",
      "Variable: x31                  Importance: 0.0\n",
      "Variable: x32                  Importance: 0.0\n",
      "Variable: x36                  Importance: 0.0\n",
      "Variable: x38                  Importance: 0.0\n",
      "Variable: x39                  Importance: 0.0\n",
      "Variable: x46                  Importance: 0.0\n",
      "Variable: x48                  Importance: 0.0\n",
      "Variable: x52                  Importance: 0.0\n",
      "Variable: x54                  Importance: 0.0\n",
      "Variable: x55                  Importance: 0.0\n",
      "Variable: x59                  Importance: 0.0\n",
      "Variable: x60                  Importance: 0.0\n",
      "Variable: x62                  Importance: 0.0\n",
      "Variable: x64                  Importance: 0.0\n",
      "Variable: x65                  Importance: 0.0\n",
      "Variable: x67                  Importance: 0.0\n",
      "Variable: x76                  Importance: 0.0\n",
      "Variable: x81                  Importance: 0.0\n",
      "Variable: x82                  Importance: 0.0\n",
      "Variable: x84                  Importance: 0.0\n",
      "Variable: x86                  Importance: 0.0\n",
      "Variable: x87                  Importance: 0.0\n",
      "Variable: x88                  Importance: 0.0\n",
      "Variable: x89                  Importance: 0.0\n",
      "Variable: x91                  Importance: 0.0\n",
      "Variable: x92                  Importance: 0.0\n",
      "Variable: x94                  Importance: 0.0\n",
      "Variable: x98                  Importance: 0.0\n",
      "Variable: x34__Honda           Importance: 0.0\n",
      "Variable: x34__Toyota          Importance: 0.0\n",
      "Variable: x34__bmw             Importance: 0.0\n",
      "Variable: x34__chevrolet       Importance: 0.0\n",
      "Variable: x34__chrystler       Importance: 0.0\n",
      "Variable: x34__ford            Importance: 0.0\n",
      "Variable: x34__mercades        Importance: 0.0\n",
      "Variable: x34__nissan          Importance: 0.0\n",
      "Variable: x34__tesla           Importance: 0.0\n",
      "Variable: x34__volkswagon      Importance: 0.0\n",
      "Variable: x35__fri             Importance: 0.0\n",
      "Variable: x35__friday          Importance: 0.0\n",
      "Variable: x35__monday          Importance: 0.0\n",
      "Variable: x35__thur            Importance: 0.0\n",
      "Variable: x35__thurday         Importance: 0.0\n",
      "Variable: x35__tuesday         Importance: 0.0\n",
      "Variable: x35__wed             Importance: 0.0\n",
      "Variable: x35__wednesday       Importance: 0.0\n",
      "Variable: x68__Apr             Importance: 0.0\n",
      "Variable: x68__Aug             Importance: 0.0\n",
      "Variable: x68__Dev             Importance: 0.0\n",
      "Variable: x68__Feb             Importance: 0.0\n",
      "Variable: x68__January         Importance: 0.0\n",
      "Variable: x68__July            Importance: 0.0\n",
      "Variable: x68__Jun             Importance: 0.0\n",
      "Variable: x68__Mar             Importance: 0.0\n",
      "Variable: x68__May             Importance: 0.0\n",
      "Variable: x68__Nov             Importance: 0.0\n",
      "Variable: x68__Oct             Importance: 0.0\n",
      "Variable: x68__sept.           Importance: 0.0\n",
      "Variable: x93__america         Importance: 0.0\n",
      "Variable: x93__asia            Importance: 0.0\n",
      "Variable: x93__euorpe          Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Random Forest variable importance using the best parameters obtained using gridsearch.\n",
    "\n",
    "# importing modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#making the instance\n",
    "model=RandomForestClassifier(n_estimators=300,min_samples_leaf=5,min_samples_split=5,criterion='entropy')\n",
    "\n",
    "#Learning\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Prediction\n",
    "prediction=model.predict(X_test)\n",
    "\n",
    "# variable importance\n",
    "\n",
    "feature_list = upsampled.columns\n",
    "# Get numerical feature importances\n",
    "importances = list(model.feature_importances_)# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-23.230884</td>\n",
       "      <td>-1.809757</td>\n",
       "      <td>12.380690</td>\n",
       "      <td>-4.101199</td>\n",
       "      <td>-60.760749</td>\n",
       "      <td>-22.957453</td>\n",
       "      <td>-1.964078</td>\n",
       "      <td>-0.631029</td>\n",
       "      <td>-4.306616</td>\n",
       "      <td>-4.694198</td>\n",
       "      <td>...</td>\n",
       "      <td>-200.759241</td>\n",
       "      <td>1.474824</td>\n",
       "      <td>-8.057325</td>\n",
       "      <td>asia</td>\n",
       "      <td>3.418412</td>\n",
       "      <td>-10.364264</td>\n",
       "      <td>-18.055341</td>\n",
       "      <td>-6.236096</td>\n",
       "      <td>-4.909211</td>\n",
       "      <td>47.679903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138.561415</td>\n",
       "      <td>1.107473</td>\n",
       "      <td>-19.781009</td>\n",
       "      <td>-17.584757</td>\n",
       "      <td>-76.922109</td>\n",
       "      <td>71.816830</td>\n",
       "      <td>-0.418432</td>\n",
       "      <td>1.403957</td>\n",
       "      <td>-5.367051</td>\n",
       "      <td>0.039857</td>\n",
       "      <td>...</td>\n",
       "      <td>-323.789253</td>\n",
       "      <td>5.539663</td>\n",
       "      <td>-1.958140</td>\n",
       "      <td>asia</td>\n",
       "      <td>-5.208792</td>\n",
       "      <td>-52.514599</td>\n",
       "      <td>-0.946514</td>\n",
       "      <td>-20.699992</td>\n",
       "      <td>-3.704683</td>\n",
       "      <td>-35.404859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.243047</td>\n",
       "      <td>-10.207303</td>\n",
       "      <td>-7.507803</td>\n",
       "      <td>3.152110</td>\n",
       "      <td>-14.915059</td>\n",
       "      <td>30.576213</td>\n",
       "      <td>-0.378178</td>\n",
       "      <td>2.606353</td>\n",
       "      <td>1.581679</td>\n",
       "      <td>4.802973</td>\n",
       "      <td>...</td>\n",
       "      <td>27.816461</td>\n",
       "      <td>-0.977431</td>\n",
       "      <td>10.324237</td>\n",
       "      <td>asia</td>\n",
       "      <td>7.939948</td>\n",
       "      <td>19.031967</td>\n",
       "      <td>7.961947</td>\n",
       "      <td>6.761312</td>\n",
       "      <td>-1.072949</td>\n",
       "      <td>36.075623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.963713</td>\n",
       "      <td>17.580528</td>\n",
       "      <td>13.884170</td>\n",
       "      <td>-17.164185</td>\n",
       "      <td>-33.547539</td>\n",
       "      <td>19.288187</td>\n",
       "      <td>-1.219019</td>\n",
       "      <td>5.574611</td>\n",
       "      <td>-3.879657</td>\n",
       "      <td>2.693111</td>\n",
       "      <td>...</td>\n",
       "      <td>-374.983643</td>\n",
       "      <td>-2.631231</td>\n",
       "      <td>2.206816</td>\n",
       "      <td>asia</td>\n",
       "      <td>-8.234822</td>\n",
       "      <td>-14.077963</td>\n",
       "      <td>-11.476360</td>\n",
       "      <td>15.808888</td>\n",
       "      <td>6.361906</td>\n",
       "      <td>47.278303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.431028</td>\n",
       "      <td>-6.232849</td>\n",
       "      <td>52.780835</td>\n",
       "      <td>-7.053903</td>\n",
       "      <td>5.679193</td>\n",
       "      <td>-29.618139</td>\n",
       "      <td>1.331833</td>\n",
       "      <td>4.425692</td>\n",
       "      <td>-4.213258</td>\n",
       "      <td>-0.398755</td>\n",
       "      <td>...</td>\n",
       "      <td>-352.113759</td>\n",
       "      <td>1.759678</td>\n",
       "      <td>-7.117473</td>\n",
       "      <td>asia</td>\n",
       "      <td>5.998549</td>\n",
       "      <td>-32.618465</td>\n",
       "      <td>-11.656808</td>\n",
       "      <td>-19.310773</td>\n",
       "      <td>-2.407052</td>\n",
       "      <td>-54.988928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x0         x1         x2         x3         x4         x5  \\\n",
       "0  -23.230884  -1.809757  12.380690  -4.101199 -60.760749 -22.957453   \n",
       "1  138.561415   1.107473 -19.781009 -17.584757 -76.922109  71.816830   \n",
       "2   -9.243047 -10.207303  -7.507803   3.152110 -14.915059  30.576213   \n",
       "3    8.963713  17.580528  13.884170 -17.164185 -33.547539  19.288187   \n",
       "4   27.431028  -6.232849  52.780835  -7.053903   5.679193 -29.618139   \n",
       "\n",
       "         x6        x7        x8        x9    ...             x90       x91  \\\n",
       "0 -1.964078 -0.631029 -4.306616 -4.694198    ...     -200.759241  1.474824   \n",
       "1 -0.418432  1.403957 -5.367051  0.039857    ...     -323.789253  5.539663   \n",
       "2 -0.378178  2.606353  1.581679  4.802973    ...       27.816461 -0.977431   \n",
       "3 -1.219019  5.574611 -3.879657  2.693111    ...     -374.983643 -2.631231   \n",
       "4  1.331833  4.425692 -4.213258 -0.398755    ...     -352.113759  1.759678   \n",
       "\n",
       "         x92   x93       x94        x95        x96        x97       x98  \\\n",
       "0  -8.057325  asia  3.418412 -10.364264 -18.055341  -6.236096 -4.909211   \n",
       "1  -1.958140  asia -5.208792 -52.514599  -0.946514 -20.699992 -3.704683   \n",
       "2  10.324237  asia  7.939948  19.031967   7.961947   6.761312 -1.072949   \n",
       "3   2.206816  asia -8.234822 -14.077963 -11.476360  15.808888  6.361906   \n",
       "4  -7.117473  asia  5.998549 -32.618465 -11.656808 -19.310773 -2.407052   \n",
       "\n",
       "         x99  \n",
       "0  47.679903  \n",
       "1 -35.404859  \n",
       "2  36.075623  \n",
       "3  47.278303  \n",
       "4 -54.988928  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test data to arry for prediction\n",
    "test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities and save to output\n",
    "\n",
    "proba = model.predict_proba(test)\n",
    "one_class =[]\n",
    "for i in range(len(proba)):\n",
    "    one_class.append(proba[i][1])\n",
    "one_class = pd.DataFrame({'one_class':one_class})\n",
    "one_class.to_excel(\"output.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rezoan Ahmed\\Anaconda5\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Rezoan Ahmed\\Anaconda5\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "32717/32717 [==============================] - 2s 51us/step - loss: 0.6309 - acc: 0.7046\n",
      "Epoch 2/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.3613 - acc: 0.8435\n",
      "Epoch 3/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.3010 - acc: 0.8774\n",
      "Epoch 4/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.2509 - acc: 0.9046\n",
      "Epoch 5/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.2080 - acc: 0.9280\n",
      "Epoch 6/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1780 - acc: 0.9428\n",
      "Epoch 7/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1562 - acc: 0.9509\n",
      "Epoch 8/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.1398 - acc: 0.9578\n",
      "Epoch 9/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.1282 - acc: 0.9622\n",
      "Epoch 10/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.1179 - acc: 0.9656\n",
      "Epoch 11/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1098 - acc: 0.9687\n",
      "Epoch 12/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1036 - acc: 0.9705\n",
      "Epoch 13/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0980 - acc: 0.9733\n",
      "Epoch 14/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0937 - acc: 0.9741\n",
      "Epoch 15/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0899 - acc: 0.9762\n",
      "Epoch 16/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0854 - acc: 0.9777\n",
      "Epoch 17/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0832 - acc: 0.9775\n",
      "Epoch 18/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0789 - acc: 0.9794\n",
      "Epoch 19/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0752 - acc: 0.9797\n",
      "Epoch 20/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0727 - acc: 0.9814\n",
      "Epoch 21/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0701 - acc: 0.9812\n",
      "Epoch 22/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0691 - acc: 0.9818\n",
      "Epoch 23/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0662 - acc: 0.9833\n",
      "Epoch 24/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0640 - acc: 0.9835\n",
      "Epoch 25/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0618 - acc: 0.9840\n",
      "Epoch 26/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0597 - acc: 0.9847\n",
      "Epoch 27/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0578 - acc: 0.9855\n",
      "Epoch 28/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0559 - acc: 0.9859\n",
      "Epoch 29/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0545 - acc: 0.9862\n",
      "Epoch 30/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0521 - acc: 0.9873\n",
      "Epoch 31/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0514 - acc: 0.9871\n",
      "Epoch 32/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0494 - acc: 0.9874\n",
      "Epoch 33/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0480 - acc: 0.9885\n",
      "Epoch 34/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0476 - acc: 0.9873\n",
      "Epoch 35/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0450 - acc: 0.9886\n",
      "Epoch 36/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0448 - acc: 0.9890\n",
      "Epoch 37/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0435 - acc: 0.9891\n",
      "Epoch 38/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0411 - acc: 0.9895\n",
      "Epoch 39/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0432 - acc: 0.9886\n",
      "Epoch 40/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0397 - acc: 0.9900\n",
      "Epoch 41/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0386 - acc: 0.9903\n",
      "Epoch 42/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0366 - acc: 0.9913\n",
      "Epoch 43/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0353 - acc: 0.9916\n",
      "Epoch 44/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0350 - acc: 0.9917\n",
      "Epoch 45/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0335 - acc: 0.9919\n",
      "Epoch 46/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0353 - acc: 0.9912\n",
      "Epoch 47/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0325 - acc: 0.9924: 1s - loss: 0.0356\n",
      "Epoch 48/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0313 - acc: 0.9928\n",
      "Epoch 49/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0302 - acc: 0.9934\n",
      "Epoch 50/100\n",
      "32717/32717 [==============================] - 1s 44us/step - loss: 0.0295 - acc: 0.9935\n",
      "Epoch 51/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0285 - acc: 0.9937\n",
      "Epoch 52/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0271 - acc: 0.9943\n",
      "Epoch 53/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0266 - acc: 0.9941\n",
      "Epoch 54/100\n",
      "32717/32717 [==============================] - 1s 45us/step - loss: 0.0283 - acc: 0.9929\n",
      "Epoch 55/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0260 - acc: 0.9942\n",
      "Epoch 56/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0249 - acc: 0.9942\n",
      "Epoch 57/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0236 - acc: 0.9952\n",
      "Epoch 58/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0227 - acc: 0.9954\n",
      "Epoch 59/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0222 - acc: 0.9954\n",
      "Epoch 60/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0219 - acc: 0.9954\n",
      "Epoch 61/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0226 - acc: 0.9948\n",
      "Epoch 62/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0214 - acc: 0.9954\n",
      "Epoch 63/100\n",
      "32717/32717 [==============================] - 1s 44us/step - loss: 0.0201 - acc: 0.9958\n",
      "Epoch 64/100\n",
      "32717/32717 [==============================] - 2s 47us/step - loss: 0.0203 - acc: 0.9957\n",
      "Epoch 65/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0202 - acc: 0.9955\n",
      "Epoch 66/100\n",
      "32717/32717 [==============================] - 2s 51us/step - loss: 0.0178 - acc: 0.9968\n",
      "Epoch 67/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0170 - acc: 0.9972\n",
      "Epoch 68/100\n",
      "32717/32717 [==============================] - 1s 37us/step - loss: 0.0165 - acc: 0.9971\n",
      "Epoch 69/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0160 - acc: 0.9971\n",
      "Epoch 70/100\n",
      "32717/32717 [==============================] - 1s 37us/step - loss: 0.0168 - acc: 0.9966\n",
      "Epoch 71/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0161 - acc: 0.9970\n",
      "Epoch 72/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0146 - acc: 0.9977\n",
      "Epoch 73/100\n",
      "32717/32717 [==============================] - 2s 48us/step - loss: 0.0142 - acc: 0.9977\n",
      "Epoch 74/100\n",
      "32717/32717 [==============================] - 2s 48us/step - loss: 0.0155 - acc: 0.9970\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0142 - acc: 0.9977\n",
      "Epoch 76/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0137 - acc: 0.9977\n",
      "Epoch 77/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0128 - acc: 0.9976\n",
      "Epoch 78/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0124 - acc: 0.9983\n",
      "Epoch 79/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0114 - acc: 0.9984\n",
      "Epoch 80/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0110 - acc: 0.9985\n",
      "Epoch 81/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0110 - acc: 0.9987\n",
      "Epoch 82/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0118 - acc: 0.9979\n",
      "Epoch 83/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0108 - acc: 0.9984\n",
      "Epoch 84/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0104 - acc: 0.9983\n",
      "Epoch 85/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0108 - acc: 0.9985\n",
      "Epoch 86/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0102 - acc: 0.9986\n",
      "Epoch 87/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0098 - acc: 0.9985\n",
      "Epoch 88/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0088 - acc: 0.9988\n",
      "Epoch 89/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0086 - acc: 0.9989\n",
      "Epoch 90/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0099 - acc: 0.9982\n",
      "Epoch 91/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0085 - acc: 0.9989\n",
      "Epoch 92/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0095 - acc: 0.9982: 0s - loss: 0.0089 - \n",
      "Epoch 93/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0078 - acc: 0.9991\n",
      "Epoch 94/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0072 - acc: 0.9993: 1s - loss: 0.009\n",
      "Epoch 95/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0072 - acc: 0.9990\n",
      "Epoch 96/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0064 - acc: 0.9993\n",
      "Epoch 97/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0060 - acc: 0.9996\n",
      "Epoch 98/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 99/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0058 - acc: 0.9995\n",
      "Epoch 100/100\n",
      "32717/32717 [==============================] - 1s 44us/step - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 1/100\n",
      "32717/32717 [==============================] - 2s 49us/step - loss: 0.7257 - acc: 0.6856\n",
      "Epoch 2/100\n",
      "32717/32717 [==============================] - 1s 37us/step - loss: 0.3750 - acc: 0.8367\n",
      "Epoch 3/100\n",
      "32717/32717 [==============================] - 1s 37us/step - loss: 0.3050 - acc: 0.8776\n",
      "Epoch 4/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.2489 - acc: 0.9082\n",
      "Epoch 5/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.2046 - acc: 0.9299\n",
      "Epoch 6/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.1741 - acc: 0.9430\n",
      "Epoch 7/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.1504 - acc: 0.9536\n",
      "Epoch 8/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.1342 - acc: 0.9602\n",
      "Epoch 9/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1220 - acc: 0.9645\n",
      "Epoch 10/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.1127 - acc: 0.9680\n",
      "Epoch 11/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.1054 - acc: 0.9710\n",
      "Epoch 12/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0990 - acc: 0.9732\n",
      "Epoch 13/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0933 - acc: 0.9745\n",
      "Epoch 14/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0888 - acc: 0.9763\n",
      "Epoch 15/100\n",
      "32717/32717 [==============================] - 1s 37us/step - loss: 0.0843 - acc: 0.9779\n",
      "Epoch 16/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0799 - acc: 0.9794\n",
      "Epoch 17/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0768 - acc: 0.9799\n",
      "Epoch 18/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0744 - acc: 0.9803\n",
      "Epoch 19/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0707 - acc: 0.9818\n",
      "Epoch 20/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0678 - acc: 0.9829\n",
      "Epoch 21/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0661 - acc: 0.9833\n",
      "Epoch 22/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0628 - acc: 0.9836\n",
      "Epoch 23/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0606 - acc: 0.9848\n",
      "Epoch 24/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0584 - acc: 0.9858\n",
      "Epoch 25/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0563 - acc: 0.9858\n",
      "Epoch 26/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0546 - acc: 0.9865\n",
      "Epoch 27/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0520 - acc: 0.9872\n",
      "Epoch 28/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0503 - acc: 0.9876\n",
      "Epoch 29/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0484 - acc: 0.9876\n",
      "Epoch 30/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0474 - acc: 0.9891: 1s - loss: 0.044\n",
      "Epoch 31/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0468 - acc: 0.9884\n",
      "Epoch 32/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0455 - acc: 0.9888\n",
      "Epoch 33/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0426 - acc: 0.9899\n",
      "Epoch 34/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0410 - acc: 0.9907\n",
      "Epoch 35/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0400 - acc: 0.9907\n",
      "Epoch 36/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0393 - acc: 0.9907\n",
      "Epoch 37/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0379 - acc: 0.9909\n",
      "Epoch 38/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0364 - acc: 0.9917\n",
      "Epoch 39/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0362 - acc: 0.9918: 0s - loss: 0.0352 - acc: 0.9\n",
      "Epoch 40/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0367 - acc: 0.9908\n",
      "Epoch 41/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0344 - acc: 0.9919\n",
      "Epoch 42/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0321 - acc: 0.9934\n",
      "Epoch 43/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0316 - acc: 0.9926\n",
      "Epoch 44/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0299 - acc: 0.9939\n",
      "Epoch 45/100\n",
      "32717/32717 [==============================] - 1s 44us/step - loss: 0.0289 - acc: 0.9936: 1s - loss: 0.0\n",
      "Epoch 46/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0291 - acc: 0.9935\n",
      "Epoch 47/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0272 - acc: 0.9947\n",
      "Epoch 48/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0276 - acc: 0.9940\n",
      "Epoch 49/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0263 - acc: 0.9941\n",
      "Epoch 50/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0263 - acc: 0.9939\n",
      "Epoch 51/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0244 - acc: 0.9951\n",
      "Epoch 52/100\n",
      "32717/32717 [==============================] - 1s 42us/step - loss: 0.0235 - acc: 0.9957\n",
      "Epoch 53/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0235 - acc: 0.9949\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0223 - acc: 0.9956\n",
      "Epoch 55/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0207 - acc: 0.9964\n",
      "Epoch 56/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0198 - acc: 0.9965\n",
      "Epoch 57/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0204 - acc: 0.9963\n",
      "Epoch 58/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0201 - acc: 0.9961\n",
      "Epoch 59/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0185 - acc: 0.9968\n",
      "Epoch 60/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0176 - acc: 0.9969\n",
      "Epoch 61/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0169 - acc: 0.9975\n",
      "Epoch 62/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0174 - acc: 0.9972\n",
      "Epoch 63/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0173 - acc: 0.9969\n",
      "Epoch 64/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0162 - acc: 0.9972\n",
      "Epoch 65/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0153 - acc: 0.9977\n",
      "Epoch 66/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0151 - acc: 0.9975\n",
      "Epoch 67/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0146 - acc: 0.9978\n",
      "Epoch 68/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0134 - acc: 0.9983\n",
      "Epoch 69/100\n",
      "32717/32717 [==============================] - 1s 43us/step - loss: 0.0130 - acc: 0.9981\n",
      "Epoch 70/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0128 - acc: 0.9982\n",
      "Epoch 71/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0129 - acc: 0.9981\n",
      "Epoch 72/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0127 - acc: 0.9979\n",
      "Epoch 73/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0118 - acc: 0.9984\n",
      "Epoch 74/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0117 - acc: 0.9980\n",
      "Epoch 75/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0109 - acc: 0.9985\n",
      "Epoch 76/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0107 - acc: 0.9984: 1s - loss: 0.008\n",
      "Epoch 77/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0105 - acc: 0.9985\n",
      "Epoch 78/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0101 - acc: 0.9986\n",
      "Epoch 79/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0095 - acc: 0.9988\n",
      "Epoch 80/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0090 - acc: 0.9989\n",
      "Epoch 81/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0097 - acc: 0.9985\n",
      "Epoch 82/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0088 - acc: 0.9989\n",
      "Epoch 83/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0079 - acc: 0.9988\n",
      "Epoch 84/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0077 - acc: 0.9993\n",
      "Epoch 85/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0079 - acc: 0.9989\n",
      "Epoch 86/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0072 - acc: 0.9993\n",
      "Epoch 87/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0073 - acc: 0.9992\n",
      "Epoch 88/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 89/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 90/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0062 - acc: 0.9994\n",
      "Epoch 91/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0058 - acc: 0.9994\n",
      "Epoch 92/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0060 - acc: 0.9994\n",
      "Epoch 93/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0060 - acc: 0.9994\n",
      "Epoch 94/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0061 - acc: 0.9992\n",
      "Epoch 95/100\n",
      "32717/32717 [==============================] - 1s 38us/step - loss: 0.0065 - acc: 0.9991\n",
      "Epoch 96/100\n",
      "32717/32717 [==============================] - 1s 39us/step - loss: 0.0050 - acc: 0.9996\n",
      "Epoch 97/100\n",
      "32717/32717 [==============================] - 1s 45us/step - loss: 0.0050 - acc: 0.9994\n",
      "Epoch 98/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0053 - acc: 0.9994\n",
      "Epoch 99/100\n",
      "32717/32717 [==============================] - 1s 40us/step - loss: 0.0047 - acc: 0.9995\n",
      "Epoch 100/100\n",
      "32717/32717 [==============================] - 1s 41us/step - loss: 0.0047 - acc: 0.9995\n",
      "Epoch 1/100\n",
      "32718/32718 [==============================] - 2s 48us/step - loss: 0.6029 - acc: 0.7114\n",
      "Epoch 2/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.3635 - acc: 0.8437\n",
      "Epoch 3/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.2945 - acc: 0.8824\n",
      "Epoch 4/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.2379 - acc: 0.9119\n",
      "Epoch 5/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.1953 - acc: 0.9326\n",
      "Epoch 6/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1662 - acc: 0.9467\n",
      "Epoch 7/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.1452 - acc: 0.9561\n",
      "Epoch 8/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1294 - acc: 0.9636\n",
      "Epoch 9/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1169 - acc: 0.9675\n",
      "Epoch 10/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1070 - acc: 0.9711\n",
      "Epoch 11/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0994 - acc: 0.9732\n",
      "Epoch 12/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0947 - acc: 0.9750\n",
      "Epoch 13/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0885 - acc: 0.9763\n",
      "Epoch 14/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0843 - acc: 0.9773\n",
      "Epoch 15/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0787 - acc: 0.9794\n",
      "Epoch 16/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0755 - acc: 0.9805\n",
      "Epoch 17/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0719 - acc: 0.9817\n",
      "Epoch 18/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0679 - acc: 0.9825\n",
      "Epoch 19/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0646 - acc: 0.9846\n",
      "Epoch 20/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0619 - acc: 0.9850\n",
      "Epoch 21/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0600 - acc: 0.9855\n",
      "Epoch 22/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0577 - acc: 0.9864\n",
      "Epoch 23/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0564 - acc: 0.9863\n",
      "Epoch 24/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0532 - acc: 0.9875\n",
      "Epoch 25/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0505 - acc: 0.9880\n",
      "Epoch 26/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0481 - acc: 0.9892\n",
      "Epoch 27/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0462 - acc: 0.9893\n",
      "Epoch 28/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0447 - acc: 0.9901\n",
      "Epoch 29/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0427 - acc: 0.9906\n",
      "Epoch 30/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0409 - acc: 0.9910\n",
      "Epoch 31/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0401 - acc: 0.9915\n",
      "Epoch 32/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0391 - acc: 0.9911\n",
      "Epoch 33/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0372 - acc: 0.9919\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0355 - acc: 0.9928\n",
      "Epoch 35/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0345 - acc: 0.9929\n",
      "Epoch 36/100\n",
      "32718/32718 [==============================] - ETA: 0s - loss: 0.0331 - acc: 0.993 - 1s 38us/step - loss: 0.0331 - acc: 0.9934\n",
      "Epoch 37/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0319 - acc: 0.9941\n",
      "Epoch 38/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0312 - acc: 0.9938\n",
      "Epoch 39/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0294 - acc: 0.9945\n",
      "Epoch 40/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0291 - acc: 0.9946\n",
      "Epoch 41/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0298 - acc: 0.9937\n",
      "Epoch 42/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0267 - acc: 0.9953\n",
      "Epoch 43/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0258 - acc: 0.9955\n",
      "Epoch 44/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0252 - acc: 0.9958\n",
      "Epoch 45/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0250 - acc: 0.9954\n",
      "Epoch 46/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0241 - acc: 0.9957\n",
      "Epoch 47/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0226 - acc: 0.9959\n",
      "Epoch 48/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0245 - acc: 0.9950\n",
      "Epoch 49/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0219 - acc: 0.9960\n",
      "Epoch 50/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0222 - acc: 0.9959\n",
      "Epoch 51/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0202 - acc: 0.9966\n",
      "Epoch 52/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0199 - acc: 0.9966\n",
      "Epoch 53/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0194 - acc: 0.9966\n",
      "Epoch 54/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0175 - acc: 0.9973\n",
      "Epoch 55/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0174 - acc: 0.9969\n",
      "Epoch 56/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0167 - acc: 0.9973\n",
      "Epoch 57/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0165 - acc: 0.9972\n",
      "Epoch 58/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0162 - acc: 0.9972\n",
      "Epoch 59/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0155 - acc: 0.9973\n",
      "Epoch 60/100\n",
      "32718/32718 [==============================] - ETA: 0s - loss: 0.0145 - acc: 0.997 - 1s 38us/step - loss: 0.0143 - acc: 0.9980\n",
      "Epoch 61/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0141 - acc: 0.9977\n",
      "Epoch 62/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0138 - acc: 0.9976\n",
      "Epoch 63/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0131 - acc: 0.9980\n",
      "Epoch 64/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0121 - acc: 0.9985\n",
      "Epoch 65/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0117 - acc: 0.9984\n",
      "Epoch 66/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0111 - acc: 0.9983\n",
      "Epoch 67/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0109 - acc: 0.9985\n",
      "Epoch 68/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0108 - acc: 0.9983\n",
      "Epoch 69/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0101 - acc: 0.9985\n",
      "Epoch 70/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0098 - acc: 0.9986\n",
      "Epoch 71/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0099 - acc: 0.9989\n",
      "Epoch 72/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 73/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0095 - acc: 0.9985\n",
      "Epoch 74/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0090 - acc: 0.9989\n",
      "Epoch 75/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0079 - acc: 0.9991\n",
      "Epoch 76/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0075 - acc: 0.9993\n",
      "Epoch 77/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0076 - acc: 0.9991\n",
      "Epoch 78/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0071 - acc: 0.9994\n",
      "Epoch 79/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0077 - acc: 0.9991\n",
      "Epoch 80/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0069 - acc: 0.9993\n",
      "Epoch 81/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0066 - acc: 0.9993\n",
      "Epoch 82/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0062 - acc: 0.9994\n",
      "Epoch 83/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 84/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0076 - acc: 0.9988\n",
      "Epoch 85/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0072 - acc: 0.9989\n",
      "Epoch 86/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0056 - acc: 0.9995\n",
      "Epoch 87/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0053 - acc: 0.9993\n",
      "Epoch 88/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0053 - acc: 0.9994\n",
      "Epoch 89/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0049 - acc: 0.9995\n",
      "Epoch 90/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0045 - acc: 0.9997\n",
      "Epoch 91/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0043 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0039 - acc: 0.9997\n",
      "Epoch 93/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0040 - acc: 0.9997\n",
      "Epoch 94/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0039 - acc: 0.9997\n",
      "Epoch 95/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0043 - acc: 0.9997\n",
      "Epoch 96/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0038 - acc: 0.9997\n",
      "Epoch 97/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0032 - acc: 0.9998\n",
      "Epoch 98/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0031 - acc: 0.9998\n",
      "Epoch 99/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 100/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0028 - acc: 0.9999\n",
      "Epoch 1/100\n",
      "32718/32718 [==============================] - 2s 49us/step - loss: 0.6362 - acc: 0.7070\n",
      "Epoch 2/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.3419 - acc: 0.8552\n",
      "Epoch 3/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.2666 - acc: 0.8953\n",
      "Epoch 4/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.2126 - acc: 0.9250\n",
      "Epoch 5/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1717 - acc: 0.9460\n",
      "Epoch 6/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1449 - acc: 0.9580\n",
      "Epoch 7/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1272 - acc: 0.9649\n",
      "Epoch 8/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1149 - acc: 0.9690\n",
      "Epoch 9/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.1045 - acc: 0.9721\n",
      "Epoch 10/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0962 - acc: 0.9741\n",
      "Epoch 11/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0888 - acc: 0.9766\n",
      "Epoch 12/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0831 - acc: 0.9778\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0778 - acc: 0.9802\n",
      "Epoch 14/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0735 - acc: 0.9807\n",
      "Epoch 15/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0685 - acc: 0.9829\n",
      "Epoch 16/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0644 - acc: 0.9840\n",
      "Epoch 17/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0604 - acc: 0.9855\n",
      "Epoch 18/100\n",
      "32718/32718 [==============================] - 2s 47us/step - loss: 0.0581 - acc: 0.9860\n",
      "Epoch 19/100\n",
      "32718/32718 [==============================] - 1s 45us/step - loss: 0.0549 - acc: 0.9871\n",
      "Epoch 20/100\n",
      "32718/32718 [==============================] - 1s 45us/step - loss: 0.0526 - acc: 0.9876\n",
      "Epoch 21/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0498 - acc: 0.9877\n",
      "Epoch 22/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0473 - acc: 0.9891\n",
      "Epoch 23/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0446 - acc: 0.9896\n",
      "Epoch 24/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0447 - acc: 0.9890\n",
      "Epoch 25/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0425 - acc: 0.9898\n",
      "Epoch 26/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0402 - acc: 0.9904\n",
      "Epoch 27/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0403 - acc: 0.9898\n",
      "Epoch 28/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0375 - acc: 0.9909\n",
      "Epoch 29/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0357 - acc: 0.9919\n",
      "Epoch 30/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0338 - acc: 0.9925\n",
      "Epoch 31/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0318 - acc: 0.9933\n",
      "Epoch 32/100\n",
      "32718/32718 [==============================] - 1s 44us/step - loss: 0.0302 - acc: 0.9939\n",
      "Epoch 33/100\n",
      "32718/32718 [==============================] - 2s 59us/step - loss: 0.0295 - acc: 0.9936\n",
      "Epoch 34/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0278 - acc: 0.9945\n",
      "Epoch 35/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0275 - acc: 0.9939\n",
      "Epoch 36/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0259 - acc: 0.9946\n",
      "Epoch 37/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0251 - acc: 0.9947\n",
      "Epoch 38/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0252 - acc: 0.9949\n",
      "Epoch 39/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0234 - acc: 0.9954\n",
      "Epoch 40/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0225 - acc: 0.9957\n",
      "Epoch 41/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0215 - acc: 0.9961\n",
      "Epoch 42/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0213 - acc: 0.9963\n",
      "Epoch 43/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0204 - acc: 0.9962\n",
      "Epoch 44/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0193 - acc: 0.9969\n",
      "Epoch 45/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0191 - acc: 0.9963\n",
      "Epoch 46/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0187 - acc: 0.9965\n",
      "Epoch 47/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0174 - acc: 0.9970\n",
      "Epoch 48/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0176 - acc: 0.9969\n",
      "Epoch 49/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0168 - acc: 0.9971\n",
      "Epoch 50/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0168 - acc: 0.9967\n",
      "Epoch 51/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0161 - acc: 0.9969\n",
      "Epoch 52/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0150 - acc: 0.9975\n",
      "Epoch 53/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0142 - acc: 0.9976\n",
      "Epoch 54/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0135 - acc: 0.9980\n",
      "Epoch 55/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0129 - acc: 0.9980\n",
      "Epoch 56/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0126 - acc: 0.9983\n",
      "Epoch 57/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0121 - acc: 0.9982\n",
      "Epoch 58/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0125 - acc: 0.9982\n",
      "Epoch 59/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0111 - acc: 0.9983\n",
      "Epoch 60/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0105 - acc: 0.9984\n",
      "Epoch 61/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0104 - acc: 0.9986\n",
      "Epoch 62/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0100 - acc: 0.9986\n",
      "Epoch 63/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0098 - acc: 0.9989\n",
      "Epoch 64/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0095 - acc: 0.9987\n",
      "Epoch 65/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0100 - acc: 0.9983\n",
      "Epoch 66/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 67/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0097 - acc: 0.9983: 0s - loss: 0.0096 - acc: 0.99\n",
      "Epoch 68/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0085 - acc: 0.9987\n",
      "Epoch 69/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0086 - acc: 0.9985\n",
      "Epoch 70/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0084 - acc: 0.9988\n",
      "Epoch 71/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0081 - acc: 0.9987\n",
      "Epoch 72/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0077 - acc: 0.9989\n",
      "Epoch 73/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0068 - acc: 0.9993\n",
      "Epoch 74/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0070 - acc: 0.9991\n",
      "Epoch 75/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0066 - acc: 0.9992\n",
      "Epoch 76/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0060 - acc: 0.9993\n",
      "Epoch 77/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0059 - acc: 0.9995\n",
      "Epoch 78/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0056 - acc: 0.9993\n",
      "Epoch 79/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0060 - acc: 0.9993\n",
      "Epoch 80/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0084 - acc: 0.9980\n",
      "Epoch 81/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0084 - acc: 0.9984\n",
      "Epoch 82/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0053 - acc: 0.9992\n",
      "Epoch 83/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0048 - acc: 0.9994\n",
      "Epoch 84/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0051 - acc: 0.9992\n",
      "Epoch 85/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0044 - acc: 0.9997\n",
      "Epoch 86/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0038 - acc: 0.9997\n",
      "Epoch 87/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0036 - acc: 0.9998\n",
      "Epoch 88/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 89/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0032 - acc: 0.9998\n",
      "Epoch 90/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 91/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 92/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 94/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 95/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0027 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 1/100\n",
      "32718/32718 [==============================] - 2s 57us/step - loss: 0.8006 - acc: 0.6714\n",
      "Epoch 2/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.3801 - acc: 0.8358\n",
      "Epoch 3/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.2994 - acc: 0.8812\n",
      "Epoch 4/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.2404 - acc: 0.9115\n",
      "Epoch 5/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1981 - acc: 0.9332\n",
      "Epoch 6/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.1686 - acc: 0.9468\n",
      "Epoch 7/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1478 - acc: 0.9549\n",
      "Epoch 8/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.1325 - acc: 0.9611\n",
      "Epoch 9/100\n",
      "32718/32718 [==============================] - 2s 47us/step - loss: 0.1200 - acc: 0.9668\n",
      "Epoch 10/100\n",
      "32718/32718 [==============================] - 2s 51us/step - loss: 0.1105 - acc: 0.9712\n",
      "Epoch 11/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.1027 - acc: 0.9722\n",
      "Epoch 12/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0955 - acc: 0.9749\n",
      "Epoch 13/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0899 - acc: 0.9770\n",
      "Epoch 14/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0852 - acc: 0.9783\n",
      "Epoch 15/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0814 - acc: 0.9785\n",
      "Epoch 16/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0768 - acc: 0.9798\n",
      "Epoch 17/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0726 - acc: 0.9814\n",
      "Epoch 18/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0693 - acc: 0.9822\n",
      "Epoch 19/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0671 - acc: 0.9826\n",
      "Epoch 20/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0645 - acc: 0.9840\n",
      "Epoch 21/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0615 - acc: 0.9843\n",
      "Epoch 22/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0590 - acc: 0.9852\n",
      "Epoch 23/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0559 - acc: 0.9862\n",
      "Epoch 24/100\n",
      "32718/32718 [==============================] - 1s 44us/step - loss: 0.0537 - acc: 0.9871\n",
      "Epoch 25/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0525 - acc: 0.9873\n",
      "Epoch 26/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0502 - acc: 0.9875\n",
      "Epoch 27/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0480 - acc: 0.9884\n",
      "Epoch 28/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0471 - acc: 0.9891\n",
      "Epoch 29/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0452 - acc: 0.9889\n",
      "Epoch 30/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0434 - acc: 0.9894\n",
      "Epoch 31/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0423 - acc: 0.9899\n",
      "Epoch 32/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0409 - acc: 0.9903\n",
      "Epoch 33/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0390 - acc: 0.9910\n",
      "Epoch 34/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0383 - acc: 0.9912\n",
      "Epoch 35/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0386 - acc: 0.9912\n",
      "Epoch 36/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0363 - acc: 0.9919\n",
      "Epoch 37/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0347 - acc: 0.9924\n",
      "Epoch 38/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0338 - acc: 0.9921\n",
      "Epoch 39/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0326 - acc: 0.9930\n",
      "Epoch 40/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0310 - acc: 0.9931\n",
      "Epoch 41/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0303 - acc: 0.9936\n",
      "Epoch 42/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0295 - acc: 0.9938\n",
      "Epoch 43/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0280 - acc: 0.9947\n",
      "Epoch 44/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0273 - acc: 0.9946\n",
      "Epoch 45/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0272 - acc: 0.9944\n",
      "Epoch 46/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0258 - acc: 0.9948\n",
      "Epoch 47/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0248 - acc: 0.9956\n",
      "Epoch 48/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0240 - acc: 0.9955\n",
      "Epoch 49/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0233 - acc: 0.9957\n",
      "Epoch 50/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0229 - acc: 0.9958\n",
      "Epoch 51/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0226 - acc: 0.9957\n",
      "Epoch 52/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0212 - acc: 0.9964\n",
      "Epoch 53/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0236 - acc: 0.9953\n",
      "Epoch 54/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0229 - acc: 0.9953\n",
      "Epoch 55/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0203 - acc: 0.9965\n",
      "Epoch 56/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0195 - acc: 0.9966\n",
      "Epoch 57/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0191 - acc: 0.9968\n",
      "Epoch 58/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0179 - acc: 0.9973\n",
      "Epoch 59/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0171 - acc: 0.9974\n",
      "Epoch 60/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0166 - acc: 0.9977\n",
      "Epoch 61/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0161 - acc: 0.9978\n",
      "Epoch 62/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0161 - acc: 0.9978\n",
      "Epoch 63/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0156 - acc: 0.9979\n",
      "Epoch 64/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0154 - acc: 0.9977\n",
      "Epoch 65/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0151 - acc: 0.9980\n",
      "Epoch 66/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0153 - acc: 0.9978\n",
      "Epoch 67/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0149 - acc: 0.9976\n",
      "Epoch 68/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0144 - acc: 0.9979\n",
      "Epoch 69/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0134 - acc: 0.9982\n",
      "Epoch 70/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0130 - acc: 0.9984\n",
      "Epoch 71/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0128 - acc: 0.9983\n",
      "Epoch 72/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0122 - acc: 0.9984\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0124 - acc: 0.9986\n",
      "Epoch 74/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0113 - acc: 0.9985\n",
      "Epoch 75/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0120 - acc: 0.9983\n",
      "Epoch 76/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0129 - acc: 0.9979\n",
      "Epoch 77/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0118 - acc: 0.9980\n",
      "Epoch 78/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0113 - acc: 0.9985\n",
      "Epoch 79/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0101 - acc: 0.9988\n",
      "Epoch 80/100\n",
      "32718/32718 [==============================] - 1s 42us/step - loss: 0.0105 - acc: 0.9985\n",
      "Epoch 81/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0099 - acc: 0.9986\n",
      "Epoch 82/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0093 - acc: 0.9990\n",
      "Epoch 83/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0090 - acc: 0.9990\n",
      "Epoch 84/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0090 - acc: 0.9988\n",
      "Epoch 85/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0086 - acc: 0.9990\n",
      "Epoch 86/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0085 - acc: 0.9991\n",
      "Epoch 87/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0090 - acc: 0.9987\n",
      "Epoch 88/100\n",
      "32718/32718 [==============================] - 1s 41us/step - loss: 0.0075 - acc: 0.9992\n",
      "Epoch 89/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0078 - acc: 0.9990\n",
      "Epoch 90/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0082 - acc: 0.9990\n",
      "Epoch 91/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0072 - acc: 0.9992\n",
      "Epoch 92/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 93/100\n",
      "32718/32718 [==============================] - 1s 37us/step - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 94/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0062 - acc: 0.9993\n",
      "Epoch 95/100\n",
      "32718/32718 [==============================] - 1s 43us/step - loss: 0.0062 - acc: 0.9993: 1s - loss: 0.0\n",
      "Epoch 96/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0060 - acc: 0.9994\n",
      "Epoch 97/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0059 - acc: 0.9993\n",
      "Epoch 98/100\n",
      "32718/32718 [==============================] - 1s 40us/step - loss: 0.0057 - acc: 0.9993\n",
      "Epoch 99/100\n",
      "32718/32718 [==============================] - 1s 39us/step - loss: 0.0055 - acc: 0.9995\n",
      "Epoch 100/100\n",
      "32718/32718 [==============================] - 1s 38us/step - loss: 0.0053 - acc: 0.9994\n",
      "Epoch 1/100\n",
      "40897/40897 [==============================] - 2s 50us/step - loss: 0.5474 - acc: 0.7482\n",
      "Epoch 2/100\n",
      "40897/40897 [==============================] - 2s 45us/step - loss: 0.3132 - acc: 0.8728\n",
      "Epoch 3/100\n",
      "40897/40897 [==============================] - 2s 48us/step - loss: 0.2310 - acc: 0.9182\n",
      "Epoch 4/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.1821 - acc: 0.9420\n",
      "Epoch 5/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.1522 - acc: 0.9543\n",
      "Epoch 6/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.1314 - acc: 0.9619\n",
      "Epoch 7/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.1179 - acc: 0.9665\n",
      "Epoch 8/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.1062 - acc: 0.9706\n",
      "Epoch 9/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0973 - acc: 0.9741\n",
      "Epoch 10/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0898 - acc: 0.9762\n",
      "Epoch 11/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0842 - acc: 0.9782\n",
      "Epoch 12/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0780 - acc: 0.9795\n",
      "Epoch 13/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0730 - acc: 0.9811\n",
      "Epoch 14/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0694 - acc: 0.9816\n",
      "Epoch 15/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0650 - acc: 0.9835\n",
      "Epoch 16/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0631 - acc: 0.9845\n",
      "Epoch 17/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0594 - acc: 0.9853\n",
      "Epoch 18/100\n",
      "40897/40897 [==============================] - 2s 43us/step - loss: 0.0563 - acc: 0.9862\n",
      "Epoch 19/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0535 - acc: 0.9867\n",
      "Epoch 20/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0508 - acc: 0.9884\n",
      "Epoch 21/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0508 - acc: 0.9870\n",
      "Epoch 22/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0475 - acc: 0.9880\n",
      "Epoch 23/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0442 - acc: 0.9895\n",
      "Epoch 24/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0434 - acc: 0.9897\n",
      "Epoch 25/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0412 - acc: 0.9905\n",
      "Epoch 26/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0388 - acc: 0.9913\n",
      "Epoch 27/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0382 - acc: 0.9908\n",
      "Epoch 28/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0365 - acc: 0.9918\n",
      "Epoch 29/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0384 - acc: 0.9906\n",
      "Epoch 30/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0351 - acc: 0.9923\n",
      "Epoch 31/100\n",
      "40897/40897 [==============================] - 2s 43us/step - loss: 0.0326 - acc: 0.9933\n",
      "Epoch 32/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0321 - acc: 0.9933\n",
      "Epoch 33/100\n",
      "40897/40897 [==============================] - 2s 38us/step - loss: 0.0299 - acc: 0.9943\n",
      "Epoch 34/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0289 - acc: 0.9944\n",
      "Epoch 35/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0288 - acc: 0.9940\n",
      "Epoch 36/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0270 - acc: 0.9946\n",
      "Epoch 37/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0260 - acc: 0.9948\n",
      "Epoch 38/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0246 - acc: 0.9954\n",
      "Epoch 39/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0249 - acc: 0.9945\n",
      "Epoch 40/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0249 - acc: 0.9945\n",
      "Epoch 41/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0237 - acc: 0.9953\n",
      "Epoch 42/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0220 - acc: 0.9956\n",
      "Epoch 43/100\n",
      "40897/40897 [==============================] - 2s 38us/step - loss: 0.0208 - acc: 0.9965\n",
      "Epoch 44/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0200 - acc: 0.9965\n",
      "Epoch 45/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0195 - acc: 0.9966\n",
      "Epoch 46/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0202 - acc: 0.9957\n",
      "Epoch 47/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0188 - acc: 0.9966\n",
      "Epoch 48/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0175 - acc: 0.9971: 1s - loss: 0.0154\n",
      "Epoch 49/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0167 - acc: 0.9973\n",
      "Epoch 50/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0162 - acc: 0.9975\n",
      "Epoch 51/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0159 - acc: 0.9974\n",
      "Epoch 52/100\n",
      "40897/40897 [==============================] - 2s 45us/step - loss: 0.0152 - acc: 0.9976\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0147 - acc: 0.9978\n",
      "Epoch 54/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0148 - acc: 0.9975\n",
      "Epoch 55/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0140 - acc: 0.9976\n",
      "Epoch 56/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0132 - acc: 0.9978\n",
      "Epoch 57/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0125 - acc: 0.9982\n",
      "Epoch 58/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0127 - acc: 0.9978\n",
      "Epoch 59/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0151 - acc: 0.9968\n",
      "Epoch 60/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0132 - acc: 0.9975\n",
      "Epoch 61/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0114 - acc: 0.9985\n",
      "Epoch 62/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0113 - acc: 0.9981\n",
      "Epoch 63/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0104 - acc: 0.9985\n",
      "Epoch 64/100\n",
      "40897/40897 [==============================] - 2s 38us/step - loss: 0.0102 - acc: 0.9984\n",
      "Epoch 65/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0097 - acc: 0.9986\n",
      "Epoch 66/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0092 - acc: 0.9986\n",
      "Epoch 67/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0094 - acc: 0.9986\n",
      "Epoch 68/100\n",
      "40897/40897 [==============================] - 2s 44us/step - loss: 0.0081 - acc: 0.9990\n",
      "Epoch 69/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0082 - acc: 0.9989\n",
      "Epoch 70/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0085 - acc: 0.9986\n",
      "Epoch 71/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0087 - acc: 0.9985\n",
      "Epoch 72/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0086 - acc: 0.9987\n",
      "Epoch 73/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0071 - acc: 0.9990\n",
      "Epoch 74/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0066 - acc: 0.9993\n",
      "Epoch 75/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0063 - acc: 0.9994\n",
      "Epoch 76/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0062 - acc: 0.9990\n",
      "Epoch 77/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0064 - acc: 0.9994\n",
      "Epoch 78/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0057 - acc: 0.9994\n",
      "Epoch 79/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0053 - acc: 0.9994\n",
      "Epoch 80/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0054 - acc: 0.9993\n",
      "Epoch 81/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0052 - acc: 0.9995\n",
      "Epoch 82/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0052 - acc: 0.9992\n",
      "Epoch 83/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0047 - acc: 0.9995\n",
      "Epoch 84/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 85/100\n",
      "40897/40897 [==============================] - 2s 43us/step - loss: 0.0055 - acc: 0.9992\n",
      "Epoch 86/100\n",
      "40897/40897 [==============================] - 2s 50us/step - loss: 0.0042 - acc: 0.9995\n",
      "Epoch 87/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 88/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 89/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0041 - acc: 0.9996\n",
      "Epoch 90/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0034 - acc: 0.9997\n",
      "Epoch 91/100\n",
      "40897/40897 [==============================] - 2s 42us/step - loss: 0.0033 - acc: 0.9997\n",
      "Epoch 92/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0040 - acc: 0.9995\n",
      "Epoch 93/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 94/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 95/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 96/100\n",
      "40897/40897 [==============================] - 2s 39us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 97/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 98/100\n",
      "40897/40897 [==============================] - 2s 41us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 99/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "40897/40897 [==============================] - 2s 40us/step - loss: 0.0030 - acc: 0.9995\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'uniform', activation = 'relu', input_dim = 129))\n",
    "    classifier.add(Dense(units = 30, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 1000, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5, n_jobs = -1)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "\n",
    "# Improving the ANN\n",
    "# Dropout Regularization to reduce overfitting if needed\n",
    "\n",
    "# Tuning the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 60, kernel_initializer = 'uniform', activation = 'relu', input_dim = 129))\n",
    "   # classifier.add(Dense(units = 30, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [1000],\n",
    "              'epochs': [100],\n",
    "              'optimizer': ['adam']}\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities and save to output\n",
    "\n",
    "proba = grid_search.predict_proba(test)\n",
    "one_class2 =[]\n",
    "for i in range(len(proba)):\n",
    "    one_class2.append(proba[i][1])\n",
    "one_class2 = pd.DataFrame({'one_class':one_class2})\n",
    "one_class2.to_excel(\"output2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000, 'epochs': 100, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy using Neural network: 0.987798616035406\n"
     ]
    }
   ],
   "source": [
    "print( 'Best accuracy using Neural network:',best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
